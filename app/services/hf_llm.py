import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

# âœ… ë¡œì»¬ ëª¨ë¸ ê²½ë¡œ (ì´ë¯¸ ë‹¤ìš´ë¡œë“œëœ ê²½ë¡œ)
MODEL_DIR = "/root/16_team/app/llama/Llama-3.1-8B-Instruct"

# âœ… ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ (ìµœì´ˆ ì‹¤í–‰ ì‹œ ëŠë¦´ ìˆ˜ ìˆìŒ)
tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)
model = AutoModelForCausalLM.from_pretrained(
    MODEL_DIR,
    torch_dtype=torch.float16
).to("cuda")


def ask_hf_llama(top5_list: list[dict]) -> str:
    prompt_text = "ë‹¤ìŒì€ ë©”ë‰´ ì¶”ì²œì„ ìœ„í•œ ë°ì´í„° ë¶„ì„ ê²°ê³¼ì´ë‹¤.\n"
    prompt_text += "ì¶”ì²œ ë©”ë‰´ 5ê°€ì§€ë¥¼ ì„ ì •í•˜ê³ , ê° ë©”ë‰´ì˜ ì¶”ì²œ ì´ìœ ë¥¼ ì„¤ëª…í•´ ë‹¬ë¼.\n"
    prompt_text += "ì„¤ëª…ì—ëŠ” ë°ì´í„° ê¸°ë°˜ ìš”ì†Œ(ê° ìš”ì†Œë³„ ê°€ì¤‘ì¹˜ ë“±) ì™¸ì—ë„ ì¼ë°˜ì ì¸ íŠ¹ì§•(ë§›, ê³„ì ˆ, ì–´ìš¸ë¦¬ëŠ” ìƒí™© ë“±)ì„ í¬í•¨í•´ë‹¬ë¼.\n"
    prompt_text += "ì„¤ëª…ì€ ì´ì•¼ê¸° í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•˜ê³ , ìˆ˜ì¹˜ëŠ” ê°€ì¤‘ì¹˜ë¡œ ëª…í™•íˆ í‘œì‹œí•´ë¼.\n\n"

    for i, item in enumerate(top5_list, 1):
        menu = item.get("menu", f"ì¶”ì²œ{i}")
        total_score = item.get("weight_sum", 0.0)

        detailed_weights = {
            k.replace("_weight", ""): v
            for k, v in item.items()
            if k.endswith("_weight")
        }

        weight_str = ', '.join([
            f"{key} ({value:.2f})" for key, value in sorted(detailed_weights.items(), key=lambda x: x[1], reverse=True)
        ])

        prompt_text += f"- ì¶”ì²œ ë©”ë‰´ {i}: {menu} (ì´í•© ê°€ì¤‘ì¹˜: {total_score:.2f})\n"
        prompt_text += f"  - ì´ ë©”ë‰´ëŠ” {weight_str}ì™€ ê°™ì€ ìš”ì†Œë¥¼ ë°˜ì˜í•˜ì—¬ ì¶”ì²œë˜ì—ˆë‹¤.\n"
        prompt_text += f"  - ë˜í•œ, {menu}ì˜ ì¼ë°˜ì ì¸ íŠ¹ì§•ê³¼ ì–´ìš¸ë¦¬ëŠ” ìƒí™©ì„ ê³ ë ¤í•´ ìì„¸íˆ ì„¤ëª…í•´ë‹¬ë¼.\n"

    prompt_text += "\n[ì§ˆë¬¸]\n"
    prompt_text += "ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê° ë©”ë‰´ê°€ ì¶”ì²œëœ ì´ìœ ë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ ì„¤ëª…í•´ì¤˜."

    print("ğŸ§  [í”„ë¡¬í”„íŠ¸ ì „ì†¡]")
    print(prompt_text)

    # âœ… í† í¬ë‚˜ì´ì§•
    inputs = tokenizer(prompt_text, return_tensors="pt").to("cuda")

    # âœ… ìƒì„±
    outputs = model.generate(
        **inputs,
        max_new_tokens=512,
        do_sample=True,
        top_p=0.9,
        temperature=0.7,
        repetition_penalty=1.2,
        eos_token_id=tokenizer.eos_token_id
    )

    # âœ… ê²°ê³¼ ë””ì½”ë”©
    full_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    only_response = full_text[len(prompt_text):].strip()

    print("ğŸ§  [LLM ì‘ë‹µ]")
    print(only_response)

    return only_response

def ask_site2_llama(top5_list, base_menu=None):
    print("ğŸ§  [Site2 í”„ë¡¬í”„íŠ¸ ì „ì†¡] ê¸°ì¤€ ë©”ë‰´:", base_menu)
    return ask_hf_llama(top5_list)